
<!-- this setup dependencies, but doesn't appear in the generated document -->
```julia; echo = false; results = "hidden"
using Distributions
using OrdinaryDiffEq
using Plots
using DEParamDistributions
```

## Generate some data

Simple SIR curve with observation every 5 timepoints, according to a
given design $d$.
Setting relatively wide priors for $R(0)$ and $\beta$.

```julia
ptrue = SIRParamDistribution(60., 0.4, 0.5, 0.1)
pdist = SIRParamDistribution(60., 0.4, TruncatedNormal(1, 2, 0, 5), 0.1)
prob = ode_problem(ptrue; save_idxs=1, saveat=5.)
inf_curve = solve(prob, Tsit5()).u
d = map(_->rand([5, 30, 100]), 1:length(inf_curve))
lf(x) = DEParamDistributions.joint_poisson(d .* x)
```
Sample resulting $y$ and plot compared to true curve

```julia
plotly()
y = rand(lf(inf_curve))
plot(inf_curve)
scatter!(y ./ d)
```

## Fit posterior using MCMC

```julia
turingfit = sample_mcmc(
    y, pdist, x->DEParamDistributions.array_poisson(d .* x); 
    saveat=5.
)
postfit = fitchain(turingfit)
mcmc_mean(postfit)
```

## Posterior expectation with sampling distribution $g = p(\theta)$

Using 10,000 samples.

```julia
    rvs = random_vars(pdist)
    pdraws = map(_->NamedTuple{keys(rvs)}(rand.(values(rvs))), 1:10_000)
    sols = prior_predict(pdraws, pdist; saveat=5.)
    W = importance_weights(y, sols, lf)
    sum(W) ≈ 1.
    μis1 = importance_mean(W, pdraws)
    importance_ess(W)
```

## IS with sampling distribution $p(\theta\mid y, d_0)$

Get some draws induced by some "conservative" initial design and approximate 
with a Gaussian distribution

```julia
lfbase(x) = DEParamDistributions.array_poisson(20 .* x)
ybase = rand.(lfbase(inf_curve))
turingfit = sample_mcmc(ybase, pdist, lfbase; saveat=5.)
postfit = fitchain(turingfit)
```

Now, simulate outbreaks using draws from this approximation, and find weights

```julia
postnames = Tuple(turingfit.name_map.parameters)
postdraw = map(eachcol(rand(postfit, 10_000))) do draw
    NamedTuple{postnames}(draw)
end
gsims = prior_predict(postdraw, pdist; saveat=5.)

# pridist = product_distribution(vcat(values(random_vars(pdist))...))
pridist = product_distribution([TruncatedNormal(1, 2, 0, 5)])
## for now, turn to vec
gdraw = map(x->vcat(values(x)...), postdraw)
W2 = importance_weights(y, gdraw, gsims; lf=lf, pd=pridist, gd=postfit)
sum(W2) ≈ 1.
μis2 = importance_mean(W2, gdraw)
importance_ess(W2)
```

## Variance in mean for given $y$

```julia
reps = map(1:100) do _
    postdraw = map(eachcol(rand(postfit, 10_000))) do draw
        NamedTuple{postnames}(draw)
    end
    gsims = prior_predict(postdraw, pdist; saveat=5.)
    gdraw = map(x->vcat(values(x)...), postdraw)
    W = importance_weights(y, gdraw, gsims; lf=lf, pd=pridist, gd=postfit)
    (importance_mean(W, gdraw), importance_ess(W))
end

```


<!-- p = scatter([μ_mcmc[1]], [μ_mcmc[2]], xaxis=((0, 0.5), "β"), yaxis=((0, 0.8), "α"), lab="MCMC")
# scatter!([prob.p[1]], [prob.p[2]], lab="Truth")

vv = values.(pdraws)

est = map(100:200:10_000) do i
    es = EnsembleSolution(sols[1:i], 0, true)
    sample_importance(y, vv[1:i], es, x->DEParamDistributions.joint_binom(20, x))
end

plot!([e[1] for e in est], [e[2] for e in est])

plot(100:200:10_000, map(x->sum((x .- μ_mcmc)), est), xlab="IS Iterations", ylab="Total Error") -->