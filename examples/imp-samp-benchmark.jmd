
```julia; echo = false; results = "hidden"
using Distributions
using OrdinaryDiffEq
using BenchmarkTools
using Plots
using DEParamDistributions
```

## Generate some data

```julia
ptrue = SIRParamDistribution(60., 0.4, 0.5, 0.1)
pdist = SIRParamDistribution(60., 0.4, TruncatedNormal(1, 2, 0, 5), 0.1)
prob = ode_problem(ptrue; save_idxs=1, saveat=5.)
inf_curve = solve(prob, Tsit5()).u
d = map(_->rand([5, 30, 100]), 1:length(inf_curve))
lf(x) = DEParamDistributions.joint_poisson(d, x)
y = rand(lf(inf_curve))
```

## Speed test using `@btime`

Current implementation using $P(\theta)$ with and without premaking distributions

```julia
function preproc1(N)
    rvs = random_vars(pdist)
    pdraws = map(_->NamedTuple{keys(rvs)}(rand.(values(rvs))), 1:N)
    sols = prior_predict(pdraws, pdist; saveat=5.)
    pdraws, sols
end

function is_simple1(y, pdraw, sols, lf)
    W = importance_weights(y, sols, lf)
    (μ=importance_mean(W, pdraw), ess=importance_ess(W))
end

p, s = preproc1(100_000);
@btime is_simple1(y, p, s, lf)
```

```julia
function preproc2(N, lf)
    rvs = random_vars(pdist)
    pdraws = map(_->NamedTuple{keys(rvs)}(rand.(values(rvs))), 1:N)
    sols = prior_predict(pdraws, pdist; saveat=5.)
    pdraws, [lf(x) for x ∈ sols]
end

function is_simple2(y, pdraw, ldists)
    W = importance_weights(y, ldists)
    (μ=importance_mean(W, pdraw), ess=importance_ess(W))
end

p, ldist = preproc2(100_000, lf);
@btime is_simple2(y, p, ldist)
```

Current implementation using $G(\theta\mid y_0, d_0)$ with and without premaking distributions

```julia
function preproc3(N)
    d₀ = fill(5, length(inf_curve))
    y₀ = rand(DEParamDistributions.joint_poisson(d₀, inf_curve))
    fit₀ = sample_mcmc(y₀, pdist, x->DEParamDistributions.array_poisson(d₀, x); save_idxs=1, saveat=5.)
    gd = fitchain(fit₀, pdist)
    rvs = random_vars(pdist)
    postdraw = map(eachcol(rand(gd, N))) do draw
        NamedTuple{keys(rvs)}(draw)
    end
    gsims = prior_predict(postdraw, pdist; save_idxs=1, saveat=5.)
    pd = joint_prior(pdist)
    gdraw = map(x->vcat(values(x)...), postdraw)
    gdraw, pd, gd, gsims, [lf(x) for x ∈ gsims]
end

function is_better1(y, gdraw, gsim, lf, pd, gd)
    W = importance_weights(y, gdraw, gsim; lf, pd, gd)
    (μ=importance_mean(W, gdraw), ess=importance_ess(W))
end

gdraw, pd, gd, gsims, ldist = preproc3(100_000);
@btime is_better1(y, gdraw, gsims, lf, pd, gd)
```

```julia
function is_better2(y, gdraw, ldists, pd, gd)
    W = importance_weights(y, gdraw; ldists, pd, gd)
    (μ=importance_mean(W, gdraw), ess=importance_ess(W))
end

@btime is_better2(y, gdraw, ldist, pd, gd)
```

## Convergence speed

### Fit posterior using MCMC

```julia
turingfit = sample_mcmc(
    y, pdist, x->DEParamDistributions.array_poisson(d .* x); 
    saveat=5.
)
postfit = fitchain(turingfit)
mcmc_mean(postfit)
```

### Posterior expectation with sampling distribution $g = p(\theta)$

Using 10,000 samples.

```julia
    rvs = random_vars(pdist)
    pdraws = map(_->NamedTuple{keys(rvs)}(rand.(values(rvs))), 1:10_000)
    sols = prior_predict(pdraws, pdist; saveat=5.)
    W = importance_weights(y, sols, lf)
    sum(W) ≈ 1.
    μis1 = importance_mean(W, pdraws)
    importance_ess(W)
```

### IS with sampling distribution $p(\theta\mid y, d_0)$

Get some draws induced by some "conservative" initial design and approximate 
with a Gaussian distribution

```julia
lfbase(x) = DEParamDistributions.array_poisson(20 .* x)
ybase = rand.(lfbase(inf_curve))
turingfit = sample_mcmc(ybase, pdist, lfbase; saveat=5.)
postfit = fitchain(turingfit)
```

Now, simulate outbreaks using draws from this approximation, and find weights

```julia
postnames = Tuple(turingfit.name_map.parameters)
postdraw = map(eachcol(rand(postfit, 10_000))) do draw
    NamedTuple{postnames}(draw)
end
gsims = prior_predict(postdraw, pdist; saveat=5.)

# pridist = product_distribution(vcat(values(random_vars(pdist))...))
pridist = product_distribution([TruncatedNormal(1, 2, 0, 5)])
## for now, turn to vec
gdraw = map(x->vcat(values(x)...), postdraw)
W2 = importance_weights(y, gdraw, gsims; lf=lf, pd=pridist, gd=postfit)
sum(W2) ≈ 1.
μis2 = importance_mean(W2, gdraw)
importance_ess(W2)
```

### Variance in mean for given $y$

```julia
reps = map(1:100) do _
    postdraw = map(eachcol(rand(postfit, 10_000))) do draw
        NamedTuple{postnames}(draw)
    end
    gsims = prior_predict(postdraw, pdist; saveat=5.)
    gdraw = map(x->vcat(values(x)...), postdraw)
    W = importance_weights(y, gdraw, gsims; lf=lf, pd=pridist, gd=postfit)
    (importance_mean(W, gdraw), importance_ess(W))
end